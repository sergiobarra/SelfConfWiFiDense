{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    # Whole raw data set\n",
    "    dataset_path = 'problem1_dataset_demo.csv'\n",
    "    print('Reading dataset', dataset_path, '...')\n",
    "    # df = pd.read_csv('problem1_dataset_demo.csv', sep=';')\n",
    "    df = pd.read_csv(dataset_path, sep=';')\n",
    "    print(' - Dataset read!')\n",
    "\n",
    "    # print(df.inf.info())  # Overview of data set\n",
    "    num_samples = len(df.index)\n",
    "    print(' - Number of samples in the data set: ', str(num_samples))\n",
    "\n",
    "    # Create new feature: mean throughput\n",
    "    df_throughput = df[['s_A', 's_B', 's_C', 's_D', 's_E', 's_F']]\n",
    "    df_mean_throughput = df_throughput.mean(axis=1)\n",
    "    df_min_throughput = df['s_min']\n",
    "    df_mean_delay = df['d_av']\n",
    "    df_max_delay = df['d_max']\n",
    "\n",
    "    # ---------- Tune input data ----------\n",
    "    # Raw input features of interest\n",
    "    df_x = df[['p_A', 'p_B', 'p_C', 'p_D', 'p_E', 'p_F',\n",
    "               'Ptx_A', 'Ptx_B', 'Ptx_C', 'Ptx_D', 'Ptx_E', 'Ptx_F']]\n",
    "\n",
    "    # Add every possible value that each feature can take (primary = [1,4])\n",
    "    # - The idea is to avoid missing some value in the input when getting the corresponding dummy variables.\n",
    "    df_x_redundant = pd.DataFrame({'p_A': [0, 1, 2, 3],\n",
    "                                   'p_B': [0, 1, 2, 3],\n",
    "                                   'p_C': [0, 1, 2, 3],\n",
    "                                   'p_D': [0, 1, 2, 3],\n",
    "                                   'p_E': [0, 1, 2, 3],\n",
    "                                   'p_F': [0, 1, 2, 3],\n",
    "                                   'Ptx_A': [0, 0, 1, 1],\n",
    "                                   'Ptx_B': [0, 0, 1, 1],\n",
    "                                   'Ptx_C': [0, 0, 1, 1],\n",
    "                                   'Ptx_D': [0, 0, 1, 1],\n",
    "                                   'Ptx_E': [0, 0, 1, 1],\n",
    "                                   'Ptx_F': [0, 0, 1, 1]})\n",
    "    # print('df_x_original:\\n', df_x_original)\n",
    "    df_x = pd.concat([df_x, df_x_redundant], sort=False, ignore_index=True)\n",
    "    # print('df_x:\\n', df_x)\n",
    "\n",
    "    # Get dummies (binary features) of the primary channel feature: e.g., p_A --> p_A1, p_A2, p_A3 and p_A4\n",
    "    df_p_A = pd.get_dummies(df_x['p_A'])\n",
    "    df_p_B = pd.get_dummies(df_x['p_B'])\n",
    "    df_p_C = pd.get_dummies(df_x['p_C'])\n",
    "    df_p_D = pd.get_dummies(df_x['p_D'])\n",
    "    df_p_E = pd.get_dummies(df_x['p_E'])\n",
    "    df_p_F = pd.get_dummies(df_x['p_F'])\n",
    "    df_p_A.rename(columns={0: 'p_A_0', 1: 'p_A_1', 2: 'p_A_2', 3: 'p_A_3'}, inplace=True)\n",
    "    df_p_B.rename(columns={0: 'p_B_0', 1: 'p_B_1', 2: 'p_B_2', 3: 'p_B_3'}, inplace=True)\n",
    "    df_p_C.rename(columns={0: 'p_C_0', 1: 'p_C_1', 2: 'p_C_2', 3: 'p_C_3'}, inplace=True)\n",
    "    df_p_D.rename(columns={0: 'p_D_0', 1: 'p_D_1', 2: 'p_D_2', 3: 'p_D_3'}, inplace=True)\n",
    "    df_p_E.rename(columns={0: 'p_E_0', 1: 'p_E_1', 2: 'p_E_2', 3: 'p_E_3'}, inplace=True)\n",
    "    df_p_F.rename(columns={0: 'p_F_0', 1: 'p_F_1', 2: 'p_F_2', 3: 'p_F_3'}, inplace=True)\n",
    "\n",
    "    df_x.loc[(df_x.Ptx_A == 14), 'Ptx_A'] = 0\n",
    "    df_x.loc[(df_x.Ptx_B == 14), 'Ptx_B'] = 0\n",
    "    df_x.loc[(df_x.Ptx_C == 14), 'Ptx_C'] = 0\n",
    "    df_x.loc[(df_x.Ptx_D == 14), 'Ptx_D'] = 0\n",
    "    df_x.loc[(df_x.Ptx_E == 14), 'Ptx_E'] = 0\n",
    "    df_x.loc[(df_x.Ptx_F == 14), 'Ptx_F'] = 0\n",
    "    df_x.loc[(df_x.Ptx_A == 20), 'Ptx_A'] = 1\n",
    "    df_x.loc[(df_x.Ptx_B == 20), 'Ptx_B'] = 1\n",
    "    df_x.loc[(df_x.Ptx_C == 20), 'Ptx_C'] = 1\n",
    "    df_x.loc[(df_x.Ptx_D == 20), 'Ptx_D'] = 1\n",
    "    df_x.loc[(df_x.Ptx_E == 20), 'Ptx_E'] = 1\n",
    "    df_x.loc[(df_x.Ptx_F == 20), 'Ptx_F'] = 1\n",
    "\n",
    "    frames = [df_p_A, df_p_B, df_p_C, df_p_D, df_p_E, df_p_F,\n",
    "              df_x['Ptx_A'], df_x['Ptx_B'], df_x['Ptx_C'], df_x['Ptx_D'], df_x['Ptx_E'], df_x['Ptx_F']]\n",
    "    df_x = pd.concat(frames, axis=1, join='inner')\n",
    "\n",
    "    # remove redundant entries\n",
    "    df_x.drop(df_x.tail(len(df_x_redundant.index)).index, inplace=True)  # drop last n rows\n",
    "\n",
    "    df_y = df_min_throughput\n",
    "\n",
    "    df_dataset = pd.concat([df_x, df_y], sort=False, ignore_index=True)\n",
    "\n",
    "    return df_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fitness(member):\n",
    "    return True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bread solutions: pick a piece of each solution and mix them\n",
    "def crossover(a, b):\n",
    "    return a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To introduce random noise\n",
    "def mutate(member):\n",
    "    return member"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create random member of population\n",
    "def create_new_member(df_dataset):\n",
    "\n",
    "    df_member = df_dataset.sample()\n",
    "    return df_member"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create new population\n",
    "def create_next_generation(population):\n",
    "    return population"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def main(number_of_iterations):\n",
    "\n",
    "    return True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}